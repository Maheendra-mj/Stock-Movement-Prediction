{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9453303,"sourceType":"datasetVersion","datasetId":5746237}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install praw","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import praw\nimport pandas as pd\nfrom datetime import datetime\nimport time\n\n# Reddit API credentials (replace with your own)\nreddit = praw.Reddit(\n    client_id='9ezXtdh1pB2PhMu5oWrd1Q',\n    client_secret='kZV7lztFgF9aUtNPzN0rS9lXReQ0xg',\n    user_agent='u/Early_Impression5129'\n)\n\nsubreddit_name = 'wallstreetbets'\nkeywords = ['buy', 'sell', 'invest', 'bullish', 'bearish']\n\n# Function to extract potential stock tickers\ndef extract_tickers(text):\n    tickers = []\n    words = text.split()\n    for word in words:\n        if word.isupper() and len(word) <= 5:  # Simplistic check for stock tickers\n            tickers.append(word)\n    return ', '.join(tickers)\n\n# Modified scrape_subreddit to include keyword filtering and ticker extraction\ndef scrape_subreddit(subreddit_name, post_limit=50000, comment_limit=20):\n    subreddit = reddit.subreddit(subreddit_name)\n    posts_data = []\n\n    for post in subreddit.new(limit=post_limit):\n        # Check if post contains any keywords\n        if any(keyword in post.title.lower() or keyword in post.selftext.lower() for keyword in keywords):\n            # Extract post data\n            post_data = {\n                'subreddit': subreddit_name,\n                'post_id': post.id,\n                'title': post.title,\n                'body': post.selftext,\n                'score': post.score,\n                'num_comments': post.num_comments,\n                'created_utc': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n                'author': str(post.author),\n                'url': post.url,\n                'tickers': extract_tickers(post.title + ' ' + post.selftext)  # Extract tickers from title and body\n            }\n            \n            # Extract comments\n            post.comments.replace_more(limit=0)  # Flatten comment tree\n            comments = []\n            for comment in post.comments.list()[:comment_limit]:\n                comments.append({\n                    'comment_id': comment.id,\n                    'comment_body': comment.body,\n                    'comment_score': comment.score,\n                    'comment_author': str(comment.author),\n                    'comment_created_utc': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n                    'tickers': extract_tickers(comment.body)  # Extract tickers from comment\n                })\n            \n            post_data['comments'] = comments\n            posts_data.append(post_data)\n        \n        # Sleep to respect Reddit's rate limits\n        time.sleep(2)\n\n    return posts_data\n\ndef main():\n    subreddits = ['stocks', 'investing', subreddit_name]\n    all_data = []\n\n    for subreddit in subreddits:\n        print(f\"Scraping r/{subreddit}...\")\n        subreddit_data = scrape_subreddit(subreddit)\n        all_data.extend(subreddit_data)\n        print(f\"Finished scraping r/{subreddit}. Total posts scraped: {len(subreddit_data)}\")\n\n    # Convert to DataFrame, including extracted tickers\n    df = pd.json_normalize(all_data, 'comments', ['subreddit', 'post_id', 'title', 'body', 'score', 'num_comments', 'created_utc', 'author', 'url', 'tickers'],\n                           record_prefix='comment_')\n\n    # Save to CSV\n    filename = f\"reddit_stock_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n    df.to_csv(filename, index=False)\n    print(f\"Data saved to {filename}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install yfinance","metadata":{"execution":{"iopub.status.busy":"2024-09-22T06:06:50.713127Z","iopub.execute_input":"2024-09-22T06:06:50.713620Z","iopub.status.idle":"2024-09-22T06:07:29.598622Z","shell.execute_reply.started":"2024-09-22T06:06:50.713572Z","shell.execute_reply":"2024-09-22T06:07:29.596956Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting yfinance\n  Downloading yfinance-0.2.43-py2.py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pandas>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2.2.2)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.10/site-packages (from yfinance) (1.26.4)\nRequirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2.32.3)\nCollecting multitasking>=0.0.7 (from yfinance)\n  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: lxml>=4.9.1 in /opt/conda/lib/python3.10/site-packages (from yfinance) (5.3.0)\nRequirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from yfinance) (3.11.0)\nRequirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2024.1)\nRequirement already satisfied: frozendict>=2.3.4 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2.4.4)\nCollecting peewee>=3.16.2 (from yfinance)\n  Downloading peewee-3.17.6.tar.gz (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.10/site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: html5lib>=1.1 in /opt/conda/lib/python3.10/site-packages (from yfinance) (1.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\nRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2024.7.4)\nDownloading yfinance-0.2.43-py2.py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\nBuilding wheels for collected packages: peewee\n  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.6-cp310-cp310-linux_x86_64.whl size=293632 sha256=a4fb59ffb8729030b768580dc647b3f662b4b123212722d2abe7f807da2fa59a\n  Stored in directory: /root/.cache/pip/wheels/4b/b9/b0/83d6e258e8f963f5ff111a2cd8c483ca59372a86e6a2535212\nSuccessfully built peewee\nInstalling collected packages: peewee, multitasking, yfinance\nSuccessfully installed multitasking-0.0.11 peewee-3.17.6 yfinance-0.2.43\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom textblob import TextBlob\nimport yfinance as yf\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.preprocessing import StandardScaler\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T06:10:22.143843Z","iopub.execute_input":"2024-09-22T06:10:22.144292Z","iopub.status.idle":"2024-09-22T06:10:22.150985Z","shell.execute_reply.started":"2024-09-22T06:10:22.144240Z","shell.execute_reply":"2024-09-22T06:10:22.149506Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n\n# Load the scraped Reddit data\ndf = pd.read_csv('/kaggle/input/50kcomments/reddit_stock_data_20240921_225227.csv')\n\ndef preprocess_data(df):\n    # Convert dates to datetime using the correct format\n    df['created_utc'] = pd.to_datetime(df['created_utc'], format='%Y-%m-%d %H:%M:%S')\n    \n    # Combine title and body for sentiment analysis\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    \n    # Perform sentiment analysis using TextBlob\n    df['sentiment'] = df['full_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    \n    # Extract time-based features after converting datetime\n    df['day_of_week'] = df['created_utc'].dt.dayofweek  # 0=Monday, 6=Sunday\n    df['hour_of_day'] = df['created_utc'].dt.hour       # 0-23 hours\n    \n    return df\n\ndef get_stock_data(symbol, start_date, end_date):\n    # Fetch historical stock data using yfinance\n    stock_data = yf.download(symbol, start=start_date, end=end_date)\n    stock_data['returns'] = stock_data['Close'].pct_change()  # Calculate daily returns\n    return stock_data\n\ndef create_features(reddit_df, stock_df):\n    # Aggregate Reddit data by date and tickers\n    daily_reddit = reddit_df.groupby([reddit_df['created_utc'].dt.date, 'tickers']).agg({\n        'sentiment': 'mean',           # Average sentiment per day\n        'score': 'sum',                # Sum of upvotes (popularity)\n        'num_comments': 'sum',         # Sum of comments (engagement)\n        'day_of_week': 'first',        # Get the first day_of_week entry for each day\n        'hour_of_day': 'first'         # Get the first hour_of_day entry for each day\n    }).reset_index()\n    \n    # Merge Reddit data with stock data\n    stock_df['created_utc'] = stock_df.index.date\n    merged_df = pd.merge(daily_reddit, stock_df, left_on='created_utc', right_on='created_utc', how='inner')\n    \n    # Calculate moving averages\n    merged_df['MA_10'] = merged_df['Close'].rolling(window=10).mean()\n    merged_df['MA_50'] = merged_df['Close'].rolling(window=50).mean()\n    \n    # Create result column based on moving average crossover strategy\n    merged_df['result'] = (merged_df['MA_10'] > merged_df['MA_50']).astype(int)\n    \n    return merged_df.dropna()\n\n\ndef create_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_dim=input_dim))\n#     model.add(Dropout(0.3))\n    model.add(Dense(32, activation='relu'))\n#     model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef train_model(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Build and train the neural network model\n    model = create_model(X_train_scaled.shape[1])\n    model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_split=0.1, verbose=2)\n    \n    # Evaluate the model\n    y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n    print(classification_report(y_test, y_pred))\n    \n    return model\n\ndef main():\n    # Load and preprocess Reddit data\n    reddit_df = preprocess_data(df)\n    \n    # Get stock data (example: AAPL)\n    start_date = reddit_df['created_utc'].min().strftime('%Y-%m-%d')\n    end_date = reddit_df['created_utc'].max().strftime('%Y-%m-%d')\n    stock_df = get_stock_data('AAPL', start_date, end_date)\n    \n    # Create features by merging Reddit and stock data\n    features_df = create_features(reddit_df, stock_df)\n    \n    # Prepare features and target variables for training\n    X = features_df[['sentiment', 'score', 'num_comments', 'day_of_week', 'hour_of_day']]\n    y = features_df['result']\n    \n    # Train the model and print results\n    model = train_model(X, y)\n    \n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T06:30:10.580808Z","iopub.execute_input":"2024-09-22T06:30:10.581296Z","iopub.status.idle":"2024-09-22T06:31:05.047281Z","shell.execute_reply.started":"2024-09-22T06:30:10.581252Z","shell.execute_reply":"2024-09-22T06:31:05.045911Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"[*********************100%***********************]  1 of 1 completed","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"45/45 - 1s - 33ms/step - accuracy: 0.5265 - loss: 0.6933 - val_accuracy: 0.5250 - val_loss: 0.6917\nEpoch 2/50\n45/45 - 0s - 3ms/step - accuracy: 0.5740 - loss: 0.6826 - val_accuracy: 0.5625 - val_loss: 0.6804\nEpoch 3/50\n45/45 - 0s - 3ms/step - accuracy: 0.5782 - loss: 0.6796 - val_accuracy: 0.5625 - val_loss: 0.6777\nEpoch 4/50\n45/45 - 0s - 3ms/step - accuracy: 0.5810 - loss: 0.6772 - val_accuracy: 0.5625 - val_loss: 0.6779\nEpoch 5/50\n45/45 - 0s - 3ms/step - accuracy: 0.5880 - loss: 0.6729 - val_accuracy: 0.5750 - val_loss: 0.6814\nEpoch 6/50\n45/45 - 0s - 3ms/step - accuracy: 0.5894 - loss: 0.6705 - val_accuracy: 0.5875 - val_loss: 0.6781\nEpoch 7/50\n45/45 - 0s - 3ms/step - accuracy: 0.5852 - loss: 0.6688 - val_accuracy: 0.5750 - val_loss: 0.6778\nEpoch 8/50\n45/45 - 0s - 3ms/step - accuracy: 0.6117 - loss: 0.6640 - val_accuracy: 0.5625 - val_loss: 0.6821\nEpoch 9/50\n45/45 - 0s - 3ms/step - accuracy: 0.6020 - loss: 0.6624 - val_accuracy: 0.5625 - val_loss: 0.6727\nEpoch 10/50\n45/45 - 0s - 3ms/step - accuracy: 0.6020 - loss: 0.6613 - val_accuracy: 0.5250 - val_loss: 0.6804\nEpoch 11/50\n45/45 - 0s - 3ms/step - accuracy: 0.6117 - loss: 0.6580 - val_accuracy: 0.5375 - val_loss: 0.6758\nEpoch 12/50\n45/45 - 0s - 3ms/step - accuracy: 0.6117 - loss: 0.6560 - val_accuracy: 0.5375 - val_loss: 0.6804\nEpoch 13/50\n45/45 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6513 - val_accuracy: 0.5375 - val_loss: 0.6815\nEpoch 14/50\n45/45 - 0s - 3ms/step - accuracy: 0.6047 - loss: 0.6500 - val_accuracy: 0.5250 - val_loss: 0.6721\nEpoch 15/50\n45/45 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6484 - val_accuracy: 0.5125 - val_loss: 0.6835\nEpoch 16/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6423 - val_accuracy: 0.5875 - val_loss: 0.6783\nEpoch 17/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6425 - val_accuracy: 0.5125 - val_loss: 0.6794\nEpoch 18/50\n45/45 - 0s - 3ms/step - accuracy: 0.6355 - loss: 0.6365 - val_accuracy: 0.5500 - val_loss: 0.6771\nEpoch 19/50\n45/45 - 0s - 3ms/step - accuracy: 0.6229 - loss: 0.6352 - val_accuracy: 0.5375 - val_loss: 0.6805\nEpoch 20/50\n45/45 - 0s - 3ms/step - accuracy: 0.6397 - loss: 0.6339 - val_accuracy: 0.5500 - val_loss: 0.6810\nEpoch 21/50\n45/45 - 0s - 3ms/step - accuracy: 0.6383 - loss: 0.6288 - val_accuracy: 0.5375 - val_loss: 0.6867\nEpoch 22/50\n45/45 - 0s - 6ms/step - accuracy: 0.6494 - loss: 0.6272 - val_accuracy: 0.5625 - val_loss: 0.6737\nEpoch 23/50\n45/45 - 0s - 3ms/step - accuracy: 0.6453 - loss: 0.6260 - val_accuracy: 0.5500 - val_loss: 0.6833\nEpoch 24/50\n45/45 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6233 - val_accuracy: 0.5625 - val_loss: 0.6827\nEpoch 25/50\n45/45 - 0s - 3ms/step - accuracy: 0.6494 - loss: 0.6197 - val_accuracy: 0.5875 - val_loss: 0.6821\nEpoch 26/50\n45/45 - 0s - 3ms/step - accuracy: 0.6606 - loss: 0.6175 - val_accuracy: 0.5375 - val_loss: 0.6873\nEpoch 27/50\n45/45 - 0s - 3ms/step - accuracy: 0.6466 - loss: 0.6161 - val_accuracy: 0.5875 - val_loss: 0.6808\nEpoch 28/50\n45/45 - 0s - 3ms/step - accuracy: 0.6564 - loss: 0.6131 - val_accuracy: 0.5625 - val_loss: 0.6807\nEpoch 29/50\n45/45 - 0s - 3ms/step - accuracy: 0.6690 - loss: 0.6110 - val_accuracy: 0.5625 - val_loss: 0.6799\nEpoch 30/50\n45/45 - 0s - 3ms/step - accuracy: 0.6564 - loss: 0.6089 - val_accuracy: 0.5375 - val_loss: 0.6905\nEpoch 31/50\n45/45 - 0s - 3ms/step - accuracy: 0.6620 - loss: 0.6057 - val_accuracy: 0.5625 - val_loss: 0.6861\nEpoch 32/50\n45/45 - 0s - 3ms/step - accuracy: 0.6453 - loss: 0.6083 - val_accuracy: 0.5500 - val_loss: 0.6813\nEpoch 33/50\n45/45 - 0s - 3ms/step - accuracy: 0.6592 - loss: 0.6092 - val_accuracy: 0.5750 - val_loss: 0.7013\nEpoch 34/50\n45/45 - 0s - 3ms/step - accuracy: 0.6536 - loss: 0.5980 - val_accuracy: 0.5625 - val_loss: 0.6987\nEpoch 35/50\n45/45 - 0s - 3ms/step - accuracy: 0.6578 - loss: 0.5981 - val_accuracy: 0.5500 - val_loss: 0.6852\nEpoch 36/50\n45/45 - 0s - 3ms/step - accuracy: 0.6746 - loss: 0.6008 - val_accuracy: 0.5500 - val_loss: 0.6935\nEpoch 37/50\n45/45 - 0s - 3ms/step - accuracy: 0.6718 - loss: 0.5977 - val_accuracy: 0.5375 - val_loss: 0.7031\nEpoch 38/50\n45/45 - 0s - 3ms/step - accuracy: 0.6620 - loss: 0.5985 - val_accuracy: 0.5500 - val_loss: 0.6967\nEpoch 39/50\n45/45 - 0s - 3ms/step - accuracy: 0.6662 - loss: 0.5926 - val_accuracy: 0.5500 - val_loss: 0.6953\nEpoch 40/50\n45/45 - 0s - 3ms/step - accuracy: 0.6704 - loss: 0.5922 - val_accuracy: 0.5500 - val_loss: 0.7000\nEpoch 41/50\n45/45 - 0s - 3ms/step - accuracy: 0.6648 - loss: 0.5916 - val_accuracy: 0.5500 - val_loss: 0.6955\nEpoch 42/50\n45/45 - 0s - 2ms/step - accuracy: 0.6802 - loss: 0.5892 - val_accuracy: 0.5500 - val_loss: 0.6975\nEpoch 43/50\n45/45 - 0s - 3ms/step - accuracy: 0.6704 - loss: 0.5911 - val_accuracy: 0.5500 - val_loss: 0.7002\nEpoch 44/50\n45/45 - 0s - 3ms/step - accuracy: 0.6746 - loss: 0.5871 - val_accuracy: 0.5625 - val_loss: 0.7063\nEpoch 45/50\n45/45 - 0s - 3ms/step - accuracy: 0.6802 - loss: 0.5875 - val_accuracy: 0.5375 - val_loss: 0.7112\nEpoch 46/50\n45/45 - 0s - 3ms/step - accuracy: 0.6872 - loss: 0.5851 - val_accuracy: 0.5500 - val_loss: 0.7057\nEpoch 47/50\n45/45 - 0s - 3ms/step - accuracy: 0.6802 - loss: 0.5828 - val_accuracy: 0.5500 - val_loss: 0.6905\nEpoch 48/50\n45/45 - 0s - 3ms/step - accuracy: 0.6774 - loss: 0.5814 - val_accuracy: 0.5500 - val_loss: 0.7047\nEpoch 49/50\n45/45 - 0s - 3ms/step - accuracy: 0.6802 - loss: 0.5797 - val_accuracy: 0.5625 - val_loss: 0.6925\nEpoch 50/50\n45/45 - 0s - 3ms/step - accuracy: 0.6941 - loss: 0.5788 - val_accuracy: 0.5500 - val_loss: 0.6983\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n              precision    recall  f1-score   support\n\n           0       0.56      0.46      0.51        91\n           1       0.61      0.70      0.65       109\n\n    accuracy                           0.59       200\n   macro avg       0.58      0.58      0.58       200\nweighted avg       0.59      0.59      0.58       200\n\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install xgboost","metadata":{"execution":{"iopub.status.busy":"2024-09-22T06:26:29.180906Z","iopub.execute_input":"2024-09-22T06:26:29.181457Z","iopub.status.idle":"2024-09-22T06:26:42.866021Z","shell.execute_reply.started":"2024-09-22T06:26:29.181389Z","shell.execute_reply":"2024-09-22T06:26:42.864430Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.14.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport yfinance as yf\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2024-09-22T06:27:43.548934Z","iopub.execute_input":"2024-09-22T06:27:43.549422Z","iopub.status.idle":"2024-09-22T06:27:43.795641Z","shell.execute_reply.started":"2024-09-22T06:27:43.549375Z","shell.execute_reply":"2024-09-22T06:27:43.794364Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n# Load the scraped Reddit data\ndf = pd.read_csv('/kaggle/input/50kcomments/reddit_stock_data_20240921_225227.csv')\n\ndef preprocess_data(df):\n    # Convert dates to datetime using the correct format\n    df['created_utc'] = pd.to_datetime(df['created_utc'], format='%Y-%m-%d %H:%M:%S')\n    \n    # Combine title and body for sentiment analysis\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    \n    # Perform sentiment analysis using TextBlob\n    df['sentiment'] = df['full_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    \n    # Extract time-based features after converting datetime\n    df['day_of_week'] = df['created_utc'].dt.dayofweek  # 0=Monday, 6=Sunday\n    df['hour_of_day'] = df['created_utc'].dt.hour       # 0-23 hours\n    \n    return df\n\ndef get_stock_data(symbol, start_date, end_date):\n    # Fetch historical stock data using yfinance\n    stock_data = yf.download(symbol, start=start_date, end=end_date)\n    stock_data['returns'] = stock_data['Close'].pct_change()  # Calculate daily returns\n    return stock_data\n\ndef create_features(reddit_df, stock_df):\n    # Aggregate Reddit data by date and tickers\n    daily_reddit = reddit_df.groupby([reddit_df['created_utc'].dt.date, 'tickers']).agg({\n        'sentiment': 'mean',\n        'score': 'sum',\n        'num_comments': 'sum',\n        'day_of_week': 'first',\n        'hour_of_day': 'first'\n    }).reset_index()\n    \n    # Merge Reddit data with stock data\n    stock_df['created_utc'] = stock_df.index.date\n    merged_df = pd.merge(daily_reddit, stock_df, left_on='created_utc', right_on='created_utc', how='inner')\n    \n    # Calculate moving averages\n    merged_df['MA_10'] = merged_df['Close'].rolling(window=10).mean()\n    merged_df['MA_50'] = merged_df['Close'].rolling(window=50).mean()\n    \n    # Create result column based on moving average crossover strategy\n    merged_df['result'] = (merged_df['MA_10'] > merged_df['MA_50']).astype(int)\n    \n    return merged_df.dropna()\n\ndef train_model(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Initialize classifiers\n    classifiers = {\n        'Logistic Regression': LogisticRegression(),\n        'Random Forest': RandomForestClassifier(),\n        'Support Vector Classifier': SVC(),\n        'Gradient Boosting': GradientBoostingClassifier(),\n        'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n    }\n    \n    # Train and evaluate each classifier\n    for name, clf in classifiers.items():\n        clf.fit(X_train_scaled, y_train)\n        y_pred = clf.predict(X_test_scaled)\n        print(f\"{name} Classification Report:\\n\", classification_report(y_test, y_pred))\n\ndef main():\n    # Load and preprocess Reddit data\n    reddit_df = preprocess_data(df)\n    \n    # Get stock data (example: AAPL)\n    start_date = reddit_df['created_utc'].min().strftime('%Y-%m-%d')\n    end_date = reddit_df['created_utc'].max().strftime('%Y-%m-%d')\n    stock_df = get_stock_data('AAPL', start_date, end_date)\n    \n    # Create features by merging Reddit and stock data\n    features_df = create_features(reddit_df, stock_df)\n    \n    # Prepare features and target variables for training\n    X = features_df[['sentiment', 'score', 'num_comments', 'day_of_week', 'hour_of_day']]\n    y = features_df['result']\n    \n    # Train the model and print results\n    train_model(X, y)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T06:27:48.188979Z","iopub.execute_input":"2024-09-22T06:27:48.189487Z","iopub.status.idle":"2024-09-22T06:28:36.479159Z","shell.execute_reply.started":"2024-09-22T06:27:48.189419Z","shell.execute_reply":"2024-09-22T06:28:36.477540Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[*********************100%***********************]  1 of 1 completed\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        91\n           1       0.54      0.97      0.69       109\n\n    accuracy                           0.53       200\n   macro avg       0.27      0.49      0.35       200\nweighted avg       0.29      0.53      0.38       200\n\nRandom Forest Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.55      0.45      0.49        91\n           1       0.60      0.69      0.64       109\n\n    accuracy                           0.58       200\n   macro avg       0.57      0.57      0.57       200\nweighted avg       0.58      0.58      0.57       200\n\nSupport Vector Classifier Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.52      0.15      0.24        91\n           1       0.55      0.88      0.68       109\n\n    accuracy                           0.55       200\n   macro avg       0.54      0.52      0.46       200\nweighted avg       0.54      0.55      0.48       200\n\nGradient Boosting Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.56      0.44      0.49        91\n           1       0.60      0.72      0.66       109\n\n    accuracy                           0.59       200\n   macro avg       0.58      0.58      0.57       200\nweighted avg       0.59      0.59      0.58       200\n\nXGBoost Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.52      0.47      0.50        91\n           1       0.59      0.64      0.62       109\n\n    accuracy                           0.56       200\n   macro avg       0.56      0.56      0.56       200\nweighted avg       0.56      0.56      0.56       200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# greedy layerwise training","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport yfinance as yf\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\n\n# Load the scraped Reddit data\ndf = pd.read_csv('/kaggle/input/50kcomments/reddit_stock_data_20240921_225227.csv')\n\ndef preprocess_data(df):\n    # Convert dates to datetime using the correct format\n    df['created_utc'] = pd.to_datetime(df['created_utc'], format='%Y-%m-%d %H:%M:%S')\n    \n    # Combine title and body for sentiment analysis\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    \n    # Perform sentiment analysis using TextBlob\n    df['sentiment'] = df['full_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    \n    # Extract time-based features after converting datetime\n    df['day_of_week'] = df['created_utc'].dt.dayofweek  # 0=Monday, 6=Sunday\n    df['hour_of_day'] = df['created_utc'].dt.hour       # 0-23 hours\n    \n    return df\n\ndef get_stock_data(symbol, start_date, end_date):\n    # Fetch historical stock data using yfinance\n    stock_data = yf.download(symbol, start=start_date, end=end_date)\n    stock_data['returns'] = stock_data['Close'].pct_change()  # Calculate daily returns\n    return stock_data\n\ndef create_features(reddit_df, stock_df):\n    # Aggregate Reddit data by date and tickers\n    daily_reddit = reddit_df.groupby([reddit_df['created_utc'].dt.date, 'tickers']).agg({\n        'sentiment': 'mean',\n        'score': 'sum',\n        'num_comments': 'sum',\n        'day_of_week': 'first',\n        'hour_of_day': 'first'\n    }).reset_index()\n    \n    # Merge Reddit data with stock data\n    stock_df['created_utc'] = stock_df.index.date\n    merged_df = pd.merge(daily_reddit, stock_df, left_on='created_utc', right_on='created_utc', how='inner')\n    \n    # Calculate moving averages\n    merged_df['MA_10'] = merged_df['Close'].rolling(window=10).mean()\n    merged_df['MA_50'] = merged_df['Close'].rolling(window=50).mean()\n    \n    # Create result column based on moving average crossover strategy\n    merged_df['result'] = (merged_df['MA_10'] > merged_df['MA_50']).astype(int)\n    \n    return merged_df.dropna()\n\ndef create_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_dim=input_dim, trainable=True))\n    model.add(Dense(32, activation='relu', trainable=False))  # Initially, freeze this layer\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndef train_layer(model, layer_index, X_train, y_train):\n    # Unfreeze the current layer\n    for i in range(len(model.layers)):\n        model.layers[i].trainable = (i == layer_index)\n    \n    model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1, verbose=2)\n\ndef train_model(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Build the model\n    model = create_model(X_train_scaled.shape[1])\n    \n    # Greedy layer-wise training\n    for i in range(len(model.layers)):\n        train_layer(model, i, X_train_scaled, y_train)\n    \n    # Evaluate the model\n    y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n    print(classification_report(y_test, y_pred))\n    \n    return model\n\ndef main():\n    # Load and preprocess Reddit data\n    reddit_df = preprocess_data(df)\n    \n    # Get stock data (example: AAPL)\n    start_date = reddit_df['created_utc'].min().strftime('%Y-%m-%d')\n    end_date = reddit_df['created_utc'].max().strftime('%Y-%m-%d')\n    stock_df = get_stock_data('AAPL', start_date, end_date)\n    \n    # Create features by merging Reddit and stock data\n    features_df = create_features(reddit_df, stock_df)\n    \n    # Prepare features and target variables for training\n    X = features_df[['sentiment', 'score', 'num_comments', 'day_of_week', 'hour_of_day']]\n    y = features_df['result']\n    \n    # Train the model and print results\n    model = train_model(X, y)\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:01:23.441354Z","iopub.execute_input":"2024-09-22T07:01:23.441995Z","iopub.status.idle":"2024-09-22T07:02:31.367690Z","shell.execute_reply.started":"2024-09-22T07:01:23.441947Z","shell.execute_reply":"2024-09-22T07:02:31.366244Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"[*********************100%***********************]  1 of 1 completed","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"45/45 - 1s - 23ms/step - accuracy: 0.5293 - loss: 0.6932 - val_accuracy: 0.6125 - val_loss: 0.6819\nEpoch 2/50\n45/45 - 0s - 3ms/step - accuracy: 0.5559 - loss: 0.6883 - val_accuracy: 0.5875 - val_loss: 0.6817\nEpoch 3/50\n45/45 - 0s - 3ms/step - accuracy: 0.5670 - loss: 0.6854 - val_accuracy: 0.5875 - val_loss: 0.6823\nEpoch 4/50\n45/45 - 0s - 3ms/step - accuracy: 0.5712 - loss: 0.6835 - val_accuracy: 0.5875 - val_loss: 0.6809\nEpoch 5/50\n45/45 - 0s - 3ms/step - accuracy: 0.5754 - loss: 0.6820 - val_accuracy: 0.5875 - val_loss: 0.6823\nEpoch 6/50\n45/45 - 0s - 2ms/step - accuracy: 0.5796 - loss: 0.6803 - val_accuracy: 0.5875 - val_loss: 0.6815\nEpoch 7/50\n45/45 - 0s - 3ms/step - accuracy: 0.5768 - loss: 0.6794 - val_accuracy: 0.5875 - val_loss: 0.6821\nEpoch 8/50\n45/45 - 0s - 3ms/step - accuracy: 0.5754 - loss: 0.6784 - val_accuracy: 0.5875 - val_loss: 0.6821\nEpoch 9/50\n45/45 - 0s - 3ms/step - accuracy: 0.5782 - loss: 0.6775 - val_accuracy: 0.5875 - val_loss: 0.6813\nEpoch 10/50\n45/45 - 0s - 3ms/step - accuracy: 0.5768 - loss: 0.6764 - val_accuracy: 0.5875 - val_loss: 0.6810\nEpoch 11/50\n45/45 - 0s - 3ms/step - accuracy: 0.5768 - loss: 0.6754 - val_accuracy: 0.5875 - val_loss: 0.6814\nEpoch 12/50\n45/45 - 0s - 3ms/step - accuracy: 0.5768 - loss: 0.6747 - val_accuracy: 0.5875 - val_loss: 0.6802\nEpoch 13/50\n45/45 - 0s - 3ms/step - accuracy: 0.5782 - loss: 0.6740 - val_accuracy: 0.5875 - val_loss: 0.6813\nEpoch 14/50\n45/45 - 0s - 3ms/step - accuracy: 0.5796 - loss: 0.6731 - val_accuracy: 0.5875 - val_loss: 0.6810\nEpoch 15/50\n45/45 - 0s - 3ms/step - accuracy: 0.5796 - loss: 0.6729 - val_accuracy: 0.5875 - val_loss: 0.6809\nEpoch 16/50\n45/45 - 0s - 3ms/step - accuracy: 0.5838 - loss: 0.6722 - val_accuracy: 0.5875 - val_loss: 0.6821\nEpoch 17/50\n45/45 - 0s - 3ms/step - accuracy: 0.5824 - loss: 0.6715 - val_accuracy: 0.5875 - val_loss: 0.6829\nEpoch 18/50\n45/45 - 0s - 3ms/step - accuracy: 0.5922 - loss: 0.6708 - val_accuracy: 0.5875 - val_loss: 0.6819\nEpoch 19/50\n45/45 - 0s - 3ms/step - accuracy: 0.5908 - loss: 0.6704 - val_accuracy: 0.5875 - val_loss: 0.6821\nEpoch 20/50\n45/45 - 0s - 3ms/step - accuracy: 0.5922 - loss: 0.6700 - val_accuracy: 0.5875 - val_loss: 0.6816\nEpoch 21/50\n45/45 - 0s - 3ms/step - accuracy: 0.5908 - loss: 0.6695 - val_accuracy: 0.5750 - val_loss: 0.6837\nEpoch 22/50\n45/45 - 0s - 3ms/step - accuracy: 0.5908 - loss: 0.6686 - val_accuracy: 0.5750 - val_loss: 0.6814\nEpoch 23/50\n45/45 - 0s - 3ms/step - accuracy: 0.5950 - loss: 0.6681 - val_accuracy: 0.5750 - val_loss: 0.6815\nEpoch 24/50\n45/45 - 0s - 3ms/step - accuracy: 0.5922 - loss: 0.6672 - val_accuracy: 0.5750 - val_loss: 0.6824\nEpoch 25/50\n45/45 - 0s - 3ms/step - accuracy: 0.6006 - loss: 0.6667 - val_accuracy: 0.5750 - val_loss: 0.6827\nEpoch 26/50\n45/45 - 0s - 3ms/step - accuracy: 0.5922 - loss: 0.6664 - val_accuracy: 0.5750 - val_loss: 0.6827\nEpoch 27/50\n45/45 - 0s - 3ms/step - accuracy: 0.5950 - loss: 0.6654 - val_accuracy: 0.5750 - val_loss: 0.6825\nEpoch 28/50\n45/45 - 0s - 3ms/step - accuracy: 0.6075 - loss: 0.6652 - val_accuracy: 0.5625 - val_loss: 0.6838\nEpoch 29/50\n45/45 - 0s - 3ms/step - accuracy: 0.6117 - loss: 0.6644 - val_accuracy: 0.5875 - val_loss: 0.6813\nEpoch 30/50\n45/45 - 0s - 3ms/step - accuracy: 0.6075 - loss: 0.6640 - val_accuracy: 0.5875 - val_loss: 0.6836\nEpoch 31/50\n45/45 - 0s - 3ms/step - accuracy: 0.6145 - loss: 0.6634 - val_accuracy: 0.5750 - val_loss: 0.6825\nEpoch 32/50\n45/45 - 0s - 3ms/step - accuracy: 0.6131 - loss: 0.6627 - val_accuracy: 0.5750 - val_loss: 0.6836\nEpoch 33/50\n45/45 - 0s - 3ms/step - accuracy: 0.6117 - loss: 0.6621 - val_accuracy: 0.5750 - val_loss: 0.6831\nEpoch 34/50\n45/45 - 0s - 3ms/step - accuracy: 0.6103 - loss: 0.6618 - val_accuracy: 0.5625 - val_loss: 0.6810\nEpoch 35/50\n45/45 - 0s - 3ms/step - accuracy: 0.6020 - loss: 0.6617 - val_accuracy: 0.5625 - val_loss: 0.6843\nEpoch 36/50\n45/45 - 0s - 3ms/step - accuracy: 0.6061 - loss: 0.6607 - val_accuracy: 0.5500 - val_loss: 0.6827\nEpoch 37/50\n45/45 - 0s - 3ms/step - accuracy: 0.6117 - loss: 0.6601 - val_accuracy: 0.5500 - val_loss: 0.6814\nEpoch 38/50\n45/45 - 0s - 2ms/step - accuracy: 0.6103 - loss: 0.6596 - val_accuracy: 0.5500 - val_loss: 0.6850\nEpoch 39/50\n45/45 - 0s - 3ms/step - accuracy: 0.6131 - loss: 0.6590 - val_accuracy: 0.5500 - val_loss: 0.6836\nEpoch 40/50\n45/45 - 0s - 3ms/step - accuracy: 0.6187 - loss: 0.6581 - val_accuracy: 0.5500 - val_loss: 0.6821\nEpoch 41/50\n45/45 - 0s - 2ms/step - accuracy: 0.6131 - loss: 0.6578 - val_accuracy: 0.5375 - val_loss: 0.6831\nEpoch 42/50\n45/45 - 0s - 3ms/step - accuracy: 0.6145 - loss: 0.6573 - val_accuracy: 0.5500 - val_loss: 0.6841\nEpoch 43/50\n45/45 - 0s - 3ms/step - accuracy: 0.6173 - loss: 0.6570 - val_accuracy: 0.5500 - val_loss: 0.6841\nEpoch 44/50\n45/45 - 0s - 3ms/step - accuracy: 0.6173 - loss: 0.6565 - val_accuracy: 0.5375 - val_loss: 0.6846\nEpoch 45/50\n45/45 - 0s - 3ms/step - accuracy: 0.6201 - loss: 0.6564 - val_accuracy: 0.5250 - val_loss: 0.6851\nEpoch 46/50\n45/45 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6555 - val_accuracy: 0.5500 - val_loss: 0.6839\nEpoch 47/50\n45/45 - 0s - 3ms/step - accuracy: 0.6173 - loss: 0.6558 - val_accuracy: 0.5125 - val_loss: 0.6854\nEpoch 48/50\n45/45 - 0s - 3ms/step - accuracy: 0.6145 - loss: 0.6545 - val_accuracy: 0.5500 - val_loss: 0.6843\nEpoch 49/50\n45/45 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6542 - val_accuracy: 0.5375 - val_loss: 0.6847\nEpoch 50/50\n45/45 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6534 - val_accuracy: 0.5375 - val_loss: 0.6850\nEpoch 1/50\n45/45 - 0s - 4ms/step - accuracy: 0.6243 - loss: 0.6532 - val_accuracy: 0.5375 - val_loss: 0.6857\nEpoch 2/50\n45/45 - 0s - 3ms/step - accuracy: 0.6159 - loss: 0.6523 - val_accuracy: 0.5375 - val_loss: 0.6849\nEpoch 3/50\n45/45 - 0s - 3ms/step - accuracy: 0.6229 - loss: 0.6520 - val_accuracy: 0.5375 - val_loss: 0.6875\nEpoch 4/50\n45/45 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6515 - val_accuracy: 0.5375 - val_loss: 0.6850\nEpoch 5/50\n45/45 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6509 - val_accuracy: 0.5375 - val_loss: 0.6857\nEpoch 6/50\n45/45 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6506 - val_accuracy: 0.5375 - val_loss: 0.6855\nEpoch 7/50\n45/45 - 0s - 2ms/step - accuracy: 0.6215 - loss: 0.6504 - val_accuracy: 0.5375 - val_loss: 0.6858\nEpoch 8/50\n45/45 - 0s - 3ms/step - accuracy: 0.6257 - loss: 0.6495 - val_accuracy: 0.5375 - val_loss: 0.6862\nEpoch 9/50\n45/45 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6493 - val_accuracy: 0.5375 - val_loss: 0.6850\nEpoch 10/50\n45/45 - 0s - 3ms/step - accuracy: 0.6257 - loss: 0.6489 - val_accuracy: 0.5375 - val_loss: 0.6871\nEpoch 11/50\n45/45 - 0s - 3ms/step - accuracy: 0.6201 - loss: 0.6484 - val_accuracy: 0.5375 - val_loss: 0.6855\nEpoch 12/50\n45/45 - 0s - 3ms/step - accuracy: 0.6327 - loss: 0.6480 - val_accuracy: 0.5375 - val_loss: 0.6856\nEpoch 13/50\n45/45 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6476 - val_accuracy: 0.5375 - val_loss: 0.6852\nEpoch 14/50\n45/45 - 0s - 3ms/step - accuracy: 0.6201 - loss: 0.6477 - val_accuracy: 0.5250 - val_loss: 0.6866\nEpoch 15/50\n45/45 - 0s - 3ms/step - accuracy: 0.6229 - loss: 0.6466 - val_accuracy: 0.5375 - val_loss: 0.6843\nEpoch 16/50\n45/45 - 0s - 3ms/step - accuracy: 0.6159 - loss: 0.6461 - val_accuracy: 0.5375 - val_loss: 0.6855\nEpoch 17/50\n45/45 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6458 - val_accuracy: 0.5375 - val_loss: 0.6850\nEpoch 18/50\n45/45 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6458 - val_accuracy: 0.5375 - val_loss: 0.6846\nEpoch 19/50\n45/45 - 0s - 3ms/step - accuracy: 0.6257 - loss: 0.6452 - val_accuracy: 0.5500 - val_loss: 0.6825\nEpoch 20/50\n45/45 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6450 - val_accuracy: 0.5375 - val_loss: 0.6854\nEpoch 21/50\n45/45 - 0s - 3ms/step - accuracy: 0.6327 - loss: 0.6442 - val_accuracy: 0.5375 - val_loss: 0.6834\nEpoch 22/50\n45/45 - 0s - 3ms/step - accuracy: 0.6229 - loss: 0.6443 - val_accuracy: 0.5375 - val_loss: 0.6829\nEpoch 23/50\n45/45 - 0s - 3ms/step - accuracy: 0.6257 - loss: 0.6440 - val_accuracy: 0.5250 - val_loss: 0.6836\nEpoch 24/50\n45/45 - 0s - 3ms/step - accuracy: 0.6229 - loss: 0.6434 - val_accuracy: 0.5375 - val_loss: 0.6850\nEpoch 25/50\n45/45 - 0s - 3ms/step - accuracy: 0.6313 - loss: 0.6433 - val_accuracy: 0.5375 - val_loss: 0.6832\nEpoch 26/50\n45/45 - 0s - 3ms/step - accuracy: 0.6201 - loss: 0.6431 - val_accuracy: 0.5375 - val_loss: 0.6833\nEpoch 27/50\n45/45 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6421 - val_accuracy: 0.5250 - val_loss: 0.6822\nEpoch 28/50\n45/45 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6423 - val_accuracy: 0.5125 - val_loss: 0.6829\nEpoch 29/50\n45/45 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6421 - val_accuracy: 0.5125 - val_loss: 0.6836\nEpoch 30/50\n45/45 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6417 - val_accuracy: 0.5250 - val_loss: 0.6826\nEpoch 31/50\n45/45 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6411 - val_accuracy: 0.5375 - val_loss: 0.6824\nEpoch 32/50\n45/45 - 0s - 3ms/step - accuracy: 0.6285 - loss: 0.6412 - val_accuracy: 0.5125 - val_loss: 0.6834\nEpoch 33/50\n45/45 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6408 - val_accuracy: 0.5250 - val_loss: 0.6812\nEpoch 34/50\n45/45 - 0s - 3ms/step - accuracy: 0.6187 - loss: 0.6404 - val_accuracy: 0.5125 - val_loss: 0.6837\nEpoch 35/50\n45/45 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6399 - val_accuracy: 0.5125 - val_loss: 0.6826\nEpoch 36/50\n45/45 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6396 - val_accuracy: 0.5125 - val_loss: 0.6833\nEpoch 37/50\n45/45 - 0s - 3ms/step - accuracy: 0.6229 - loss: 0.6394 - val_accuracy: 0.5250 - val_loss: 0.6819\nEpoch 38/50\n45/45 - 0s - 3ms/step - accuracy: 0.6201 - loss: 0.6393 - val_accuracy: 0.5125 - val_loss: 0.6839\nEpoch 39/50\n45/45 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6390 - val_accuracy: 0.5000 - val_loss: 0.6825\nEpoch 40/50\n45/45 - 0s - 3ms/step - accuracy: 0.6285 - loss: 0.6385 - val_accuracy: 0.5125 - val_loss: 0.6830\nEpoch 41/50\n45/45 - 0s - 3ms/step - accuracy: 0.6313 - loss: 0.6383 - val_accuracy: 0.5125 - val_loss: 0.6830\nEpoch 42/50\n45/45 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6380 - val_accuracy: 0.5250 - val_loss: 0.6830\nEpoch 43/50\n45/45 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6382 - val_accuracy: 0.5000 - val_loss: 0.6834\nEpoch 44/50\n45/45 - 0s - 3ms/step - accuracy: 0.6355 - loss: 0.6375 - val_accuracy: 0.5125 - val_loss: 0.6834\nEpoch 45/50\n45/45 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6370 - val_accuracy: 0.5000 - val_loss: 0.6846\nEpoch 46/50\n45/45 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6367 - val_accuracy: 0.5000 - val_loss: 0.6837\nEpoch 47/50\n45/45 - 0s - 3ms/step - accuracy: 0.6257 - loss: 0.6368 - val_accuracy: 0.5125 - val_loss: 0.6836\nEpoch 48/50\n45/45 - 0s - 3ms/step - accuracy: 0.6355 - loss: 0.6362 - val_accuracy: 0.5000 - val_loss: 0.6836\nEpoch 49/50\n45/45 - 0s - 3ms/step - accuracy: 0.6369 - loss: 0.6360 - val_accuracy: 0.5000 - val_loss: 0.6835\nEpoch 50/50\n45/45 - 0s - 2ms/step - accuracy: 0.6369 - loss: 0.6358 - val_accuracy: 0.5125 - val_loss: 0.6847\nEpoch 1/50\n45/45 - 0s - 4ms/step - accuracy: 0.6299 - loss: 0.6356 - val_accuracy: 0.5250 - val_loss: 0.6823\nEpoch 2/50\n45/45 - 0s - 3ms/step - accuracy: 0.6453 - loss: 0.6355 - val_accuracy: 0.5000 - val_loss: 0.6837\nEpoch 3/50\n45/45 - 0s - 3ms/step - accuracy: 0.6355 - loss: 0.6351 - val_accuracy: 0.5125 - val_loss: 0.6824\nEpoch 4/50\n45/45 - 0s - 3ms/step - accuracy: 0.6313 - loss: 0.6347 - val_accuracy: 0.5250 - val_loss: 0.6831\nEpoch 5/50\n45/45 - 0s - 3ms/step - accuracy: 0.6313 - loss: 0.6346 - val_accuracy: 0.5000 - val_loss: 0.6846\nEpoch 6/50\n45/45 - 0s - 3ms/step - accuracy: 0.6355 - loss: 0.6340 - val_accuracy: 0.5125 - val_loss: 0.6826\nEpoch 7/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6340 - val_accuracy: 0.5125 - val_loss: 0.6823\nEpoch 8/50\n45/45 - 0s - 3ms/step - accuracy: 0.6327 - loss: 0.6338 - val_accuracy: 0.5125 - val_loss: 0.6824\nEpoch 9/50\n45/45 - 0s - 3ms/step - accuracy: 0.6397 - loss: 0.6336 - val_accuracy: 0.5000 - val_loss: 0.6832\nEpoch 10/50\n45/45 - 0s - 3ms/step - accuracy: 0.6397 - loss: 0.6331 - val_accuracy: 0.5000 - val_loss: 0.6839\nEpoch 11/50\n45/45 - 0s - 3ms/step - accuracy: 0.6425 - loss: 0.6329 - val_accuracy: 0.5000 - val_loss: 0.6838\nEpoch 12/50\n45/45 - 0s - 3ms/step - accuracy: 0.6383 - loss: 0.6329 - val_accuracy: 0.5000 - val_loss: 0.6837\nEpoch 13/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6325 - val_accuracy: 0.5125 - val_loss: 0.6829\nEpoch 14/50\n45/45 - 0s - 3ms/step - accuracy: 0.6397 - loss: 0.6323 - val_accuracy: 0.5000 - val_loss: 0.6831\nEpoch 15/50\n45/45 - 0s - 3ms/step - accuracy: 0.6383 - loss: 0.6320 - val_accuracy: 0.5125 - val_loss: 0.6832\nEpoch 16/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6320 - val_accuracy: 0.5125 - val_loss: 0.6819\nEpoch 17/50\n45/45 - 0s - 3ms/step - accuracy: 0.6383 - loss: 0.6316 - val_accuracy: 0.5125 - val_loss: 0.6817\nEpoch 18/50\n45/45 - 0s - 2ms/step - accuracy: 0.6480 - loss: 0.6314 - val_accuracy: 0.5000 - val_loss: 0.6844\nEpoch 19/50\n45/45 - 0s - 2ms/step - accuracy: 0.6355 - loss: 0.6311 - val_accuracy: 0.5125 - val_loss: 0.6825\nEpoch 20/50\n45/45 - 0s - 2ms/step - accuracy: 0.6425 - loss: 0.6308 - val_accuracy: 0.5125 - val_loss: 0.6843\nEpoch 21/50\n45/45 - 0s - 3ms/step - accuracy: 0.6383 - loss: 0.6305 - val_accuracy: 0.5125 - val_loss: 0.6833\nEpoch 22/50\n45/45 - 0s - 3ms/step - accuracy: 0.6439 - loss: 0.6303 - val_accuracy: 0.5125 - val_loss: 0.6844\nEpoch 23/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6307 - val_accuracy: 0.5125 - val_loss: 0.6824\nEpoch 24/50\n45/45 - 0s - 3ms/step - accuracy: 0.6439 - loss: 0.6298 - val_accuracy: 0.5000 - val_loss: 0.6842\nEpoch 25/50\n45/45 - 0s - 3ms/step - accuracy: 0.6453 - loss: 0.6297 - val_accuracy: 0.5000 - val_loss: 0.6819\nEpoch 26/50\n45/45 - 0s - 3ms/step - accuracy: 0.6369 - loss: 0.6294 - val_accuracy: 0.5000 - val_loss: 0.6836\nEpoch 27/50\n45/45 - 0s - 2ms/step - accuracy: 0.6411 - loss: 0.6294 - val_accuracy: 0.5000 - val_loss: 0.6826\nEpoch 28/50\n45/45 - 0s - 3ms/step - accuracy: 0.6508 - loss: 0.6288 - val_accuracy: 0.5125 - val_loss: 0.6826\nEpoch 29/50\n45/45 - 0s - 3ms/step - accuracy: 0.6466 - loss: 0.6289 - val_accuracy: 0.5000 - val_loss: 0.6831\nEpoch 30/50\n45/45 - 0s - 3ms/step - accuracy: 0.6466 - loss: 0.6286 - val_accuracy: 0.5125 - val_loss: 0.6832\nEpoch 31/50\n45/45 - 0s - 3ms/step - accuracy: 0.6425 - loss: 0.6284 - val_accuracy: 0.5000 - val_loss: 0.6834\nEpoch 32/50\n45/45 - 0s - 2ms/step - accuracy: 0.6439 - loss: 0.6280 - val_accuracy: 0.5000 - val_loss: 0.6833\nEpoch 33/50\n45/45 - 0s - 2ms/step - accuracy: 0.6439 - loss: 0.6282 - val_accuracy: 0.5000 - val_loss: 0.6829\nEpoch 34/50\n45/45 - 0s - 2ms/step - accuracy: 0.6425 - loss: 0.6279 - val_accuracy: 0.5000 - val_loss: 0.6823\nEpoch 35/50\n45/45 - 0s - 3ms/step - accuracy: 0.6466 - loss: 0.6274 - val_accuracy: 0.5000 - val_loss: 0.6833\nEpoch 36/50\n45/45 - 0s - 3ms/step - accuracy: 0.6508 - loss: 0.6271 - val_accuracy: 0.5000 - val_loss: 0.6826\nEpoch 37/50\n45/45 - 0s - 3ms/step - accuracy: 0.6480 - loss: 0.6270 - val_accuracy: 0.5000 - val_loss: 0.6826\nEpoch 38/50\n45/45 - 0s - 7ms/step - accuracy: 0.6439 - loss: 0.6269 - val_accuracy: 0.5000 - val_loss: 0.6825\nEpoch 39/50\n45/45 - 0s - 3ms/step - accuracy: 0.6425 - loss: 0.6269 - val_accuracy: 0.5000 - val_loss: 0.6844\nEpoch 40/50\n45/45 - 0s - 6ms/step - accuracy: 0.6466 - loss: 0.6263 - val_accuracy: 0.5000 - val_loss: 0.6822\nEpoch 41/50\n45/45 - 0s - 2ms/step - accuracy: 0.6466 - loss: 0.6263 - val_accuracy: 0.5000 - val_loss: 0.6811\nEpoch 42/50\n45/45 - 0s - 3ms/step - accuracy: 0.6397 - loss: 0.6258 - val_accuracy: 0.5000 - val_loss: 0.6817\nEpoch 43/50\n45/45 - 0s - 3ms/step - accuracy: 0.6494 - loss: 0.6258 - val_accuracy: 0.5000 - val_loss: 0.6821\nEpoch 44/50\n45/45 - 0s - 3ms/step - accuracy: 0.6466 - loss: 0.6257 - val_accuracy: 0.5000 - val_loss: 0.6826\nEpoch 45/50\n45/45 - 0s - 3ms/step - accuracy: 0.6453 - loss: 0.6251 - val_accuracy: 0.5000 - val_loss: 0.6822\nEpoch 46/50\n45/45 - 0s - 3ms/step - accuracy: 0.6411 - loss: 0.6251 - val_accuracy: 0.5000 - val_loss: 0.6825\nEpoch 47/50\n45/45 - 0s - 3ms/step - accuracy: 0.6508 - loss: 0.6248 - val_accuracy: 0.5000 - val_loss: 0.6816\nEpoch 48/50\n45/45 - 0s - 3ms/step - accuracy: 0.6550 - loss: 0.6252 - val_accuracy: 0.5000 - val_loss: 0.6820\nEpoch 49/50\n45/45 - 0s - 3ms/step - accuracy: 0.6480 - loss: 0.6247 - val_accuracy: 0.5000 - val_loss: 0.6813\nEpoch 50/50\n45/45 - 0s - 3ms/step - accuracy: 0.6480 - loss: 0.6242 - val_accuracy: 0.5000 - val_loss: 0.6816\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n              precision    recall  f1-score   support\n\n           0       0.54      0.27      0.36        91\n           1       0.57      0.81      0.67       109\n\n    accuracy                           0.56       200\n   macro avg       0.56      0.54      0.52       200\nweighted avg       0.56      0.56      0.53       200\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:48:29.088643Z","iopub.execute_input":"2024-09-22T11:48:29.089029Z","iopub.status.idle":"2024-09-22T11:48:42.241190Z","shell.execute_reply.started":"2024-09-22T11:48:29.088992Z","shell.execute_reply":"2024-09-22T11:48:42.240236Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Bert tokenizer","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch\n!pip install yfinance","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:31:59.557199Z","iopub.execute_input":"2024-09-22T13:31:59.557526Z","iopub.status.idle":"2024-09-22T13:32:49.520504Z","shell.execute_reply.started":"2024-09-22T13:31:59.557489Z","shell.execute_reply":"2024-09-22T13:32:49.519252Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting yfinance\n  Downloading yfinance-0.2.43-py2.py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pandas>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2.2.2)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.10/site-packages (from yfinance) (1.26.4)\nRequirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2.32.3)\nCollecting multitasking>=0.0.7 (from yfinance)\n  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: lxml>=4.9.1 in /opt/conda/lib/python3.10/site-packages (from yfinance) (5.3.0)\nRequirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from yfinance) (3.11.0)\nRequirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2024.1)\nRequirement already satisfied: frozendict>=2.3.4 in /opt/conda/lib/python3.10/site-packages (from yfinance) (2.4.4)\nCollecting peewee>=3.16.2 (from yfinance)\n  Downloading peewee-3.17.6.tar.gz (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.10/site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: html5lib>=1.1 in /opt/conda/lib/python3.10/site-packages (from yfinance) (1.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\nRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2024.7.4)\nDownloading yfinance-0.2.43-py2.py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\nBuilding wheels for collected packages: peewee\n  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.6-cp310-cp310-linux_x86_64.whl size=275728 sha256=8f2b19288914a624f12522ea4b4f67b6f6e31ce4ed1e25f2f7d5276db5ca208d\n  Stored in directory: /root/.cache/pip/wheels/4b/b9/b0/83d6e258e8f963f5ff111a2cd8c483ca59372a86e6a2535212\nSuccessfully built peewee\nInstalling collected packages: peewee, multitasking, yfinance\nSuccessfully installed multitasking-0.0.11 peewee-3.17.6 yfinance-0.2.43\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom peft import get_peft_model, LoraConfig, TaskType","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:48:42.242936Z","iopub.execute_input":"2024-09-22T11:48:42.243257Z","iopub.status.idle":"2024-09-22T11:48:42.419856Z","shell.execute_reply.started":"2024-09-22T11:48:42.243224Z","shell.execute_reply":"2024-09-22T11:48:42.418973Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport yfinance as yf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load the scraped Reddit data\ndf = pd.read_csv('/kaggle/input/50kcomments/reddit_stock_data_20240921_225227.csv')\n\ndef preprocess_data(df):\n    # Convert dates to datetime using the correct format\n    df['created_utc'] = pd.to_datetime(df['created_utc'], format='%Y-%m-%d %H:%M:%S')\n    \n    # Combine title and body for sentiment analysis\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    \n    # Extract time-based features after converting datetime\n    df['day_of_week'] = df['created_utc'].dt.dayofweek  # 0=Monday, 6=Sunday\n    df['hour_of_day'] = df['created_utc'].dt.hour       # 0-23 hours\n    \n    return df\n\ndef analyze_sentiment(texts, batch_size=16):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=2,\n        torch_dtype=torch.float16\n    ).to(device)\n\n    dataset = TensorDataset(torch.arange(len(texts)))\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Analyzing sentiment\"):\n            batch_texts = [texts[i] for i in batch[0]]\n            inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            predictions.extend(preds.cpu().numpy().tolist())\n\n    return predictions\n\ndef get_stock_data(symbol, start_date, end_date):\n    stock_data = yf.download(symbol, start=start_date, end=end_date)\n    stock_data['returns'] = stock_data['Close'].pct_change()\n    return stock_data\n\ndef create_features(reddit_df, stock_df):\n    # Ensure 'day_of_week' and 'hour_of_day' are present in reddit_df after preprocessing\n    if 'day_of_week' not in reddit_df.columns or 'hour_of_day' not in reddit_df.columns:\n        raise KeyError(\"Columns ['day_of_week', 'hour_of_day'] are missing from reddit_df\")\n    \n    # Aggregate Reddit data by date and tickers\n    daily_reddit = reddit_df.groupby([reddit_df['created_utc'].dt.date, 'tickers']).agg({\n        'sentiment': 'mean',\n        'score': 'sum',\n        'num_comments': 'sum',\n        'day_of_week': 'first',   # Ensure the first day_of_week per group is taken\n        'hour_of_day': 'first'    # Ensure the first hour_of_day per group is taken\n    }).reset_index()\n    \n    stock_df['created_utc'] = stock_df.index.date\n    merged_df = pd.merge(daily_reddit, stock_df, left_on='created_utc', right_on='created_utc', how='inner')\n    \n    # Calculate moving averages\n    merged_df['MA_10'] = merged_df['Close'].rolling(window=10).mean()\n    merged_df['MA_50'] = merged_df['Close'].rolling(window=50).mean()\n    \n    # Create result column based on moving average crossover strategy\n    merged_df['result'] = (merged_df['MA_10'] > merged_df['MA_50']).astype(int)\n    \n    return merged_df.dropna()\n\ndef create_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_dim=input_dim))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndef train_model(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    model = create_model(X_train_scaled.shape[1])\n    model.fit(X_train_scaled, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=2)\n    \n    y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n    print(classification_report(y_test, y_pred))\n    \n    return model\n\ndef main():\n    print(\"Preprocessing data...\")\n    reddit_df = preprocess_data(df)\n    \n    print(\"Analyzing sentiment...\")\n    reddit_df['sentiment'] = analyze_sentiment(reddit_df['full_text'].tolist())\n    \n    start_date = reddit_df['created_utc'].min().strftime('%Y-%m-%d')\n    end_date = reddit_df['created_utc'].max().strftime('%Y-%m-%d')\n    print(f\"Fetching stock data for AAPL from {start_date} to {end_date}...\")\n    stock_df = get_stock_data('AAPL', start_date, end_date)\n    \n    print(\"Creating features...\")\n    features_df = create_features(reddit_df, stock_df)\n    \n    X = features_df[['sentiment', 'score', 'num_comments', 'day_of_week', 'hour_of_day']]\n    y = features_df['result']\n    \n    print(\"Training model...\")\n    model = train_model(X, y)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:14:39.129146Z","iopub.execute_input":"2024-09-22T12:14:39.129918Z","iopub.status.idle":"2024-09-22T12:19:17.894508Z","shell.execute_reply.started":"2024-09-22T12:14:39.129876Z","shell.execute_reply":"2024-09-22T12:19:17.893536Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Preprocessing data...\nAnalyzing sentiment...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAnalyzing sentiment: 100%|██████████| 1454/1454 [04:31<00:00,  5.36it/s]\n[*********************100%***********************]  1 of 1 completed\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Fetching stock data for AAPL from 2024-07-24 to 2024-09-21...\nCreating features...\nTraining model...\nEpoch 1/30\n23/23 - 3s - 140ms/step - accuracy: 0.5223 - loss: 0.6954 - val_accuracy: 0.5875 - val_loss: 0.6817\nEpoch 2/30\n23/23 - 0s - 4ms/step - accuracy: 0.5698 - loss: 0.6867 - val_accuracy: 0.5750 - val_loss: 0.6841\nEpoch 3/30\n23/23 - 0s - 4ms/step - accuracy: 0.5852 - loss: 0.6831 - val_accuracy: 0.5875 - val_loss: 0.6809\nEpoch 4/30\n23/23 - 0s - 4ms/step - accuracy: 0.5768 - loss: 0.6805 - val_accuracy: 0.5875 - val_loss: 0.6768\nEpoch 5/30\n23/23 - 0s - 3ms/step - accuracy: 0.5740 - loss: 0.6781 - val_accuracy: 0.6000 - val_loss: 0.6809\nEpoch 6/30\n23/23 - 0s - 4ms/step - accuracy: 0.5894 - loss: 0.6741 - val_accuracy: 0.6000 - val_loss: 0.6744\nEpoch 7/30\n23/23 - 0s - 3ms/step - accuracy: 0.5810 - loss: 0.6710 - val_accuracy: 0.5750 - val_loss: 0.6770\nEpoch 8/30\n23/23 - 0s - 6ms/step - accuracy: 0.6117 - loss: 0.6688 - val_accuracy: 0.5500 - val_loss: 0.6767\nEpoch 9/30\n23/23 - 0s - 3ms/step - accuracy: 0.6075 - loss: 0.6637 - val_accuracy: 0.5625 - val_loss: 0.6754\nEpoch 10/30\n23/23 - 0s - 3ms/step - accuracy: 0.6145 - loss: 0.6592 - val_accuracy: 0.5250 - val_loss: 0.6770\nEpoch 11/30\n23/23 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6551 - val_accuracy: 0.5500 - val_loss: 0.6754\nEpoch 12/30\n23/23 - 0s - 3ms/step - accuracy: 0.6215 - loss: 0.6488 - val_accuracy: 0.5375 - val_loss: 0.6787\nEpoch 13/30\n23/23 - 0s - 3ms/step - accuracy: 0.6103 - loss: 0.6472 - val_accuracy: 0.5250 - val_loss: 0.6848\nEpoch 14/30\n23/23 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6388 - val_accuracy: 0.5250 - val_loss: 0.6769\nEpoch 15/30\n23/23 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6367 - val_accuracy: 0.5250 - val_loss: 0.6784\nEpoch 16/30\n23/23 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6326 - val_accuracy: 0.5250 - val_loss: 0.6858\nEpoch 17/30\n23/23 - 0s - 3ms/step - accuracy: 0.6201 - loss: 0.6289 - val_accuracy: 0.5250 - val_loss: 0.6754\nEpoch 18/30\n23/23 - 0s - 3ms/step - accuracy: 0.6271 - loss: 0.6246 - val_accuracy: 0.5250 - val_loss: 0.6791\nEpoch 19/30\n23/23 - 0s - 3ms/step - accuracy: 0.6285 - loss: 0.6214 - val_accuracy: 0.5250 - val_loss: 0.6743\nEpoch 20/30\n23/23 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6189 - val_accuracy: 0.5375 - val_loss: 0.6804\nEpoch 21/30\n23/23 - 0s - 3ms/step - accuracy: 0.6313 - loss: 0.6151 - val_accuracy: 0.5250 - val_loss: 0.6755\nEpoch 22/30\n23/23 - 0s - 4ms/step - accuracy: 0.6299 - loss: 0.6160 - val_accuracy: 0.5375 - val_loss: 0.6832\nEpoch 23/30\n23/23 - 0s - 3ms/step - accuracy: 0.6243 - loss: 0.6150 - val_accuracy: 0.5375 - val_loss: 0.6862\nEpoch 24/30\n23/23 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6161 - val_accuracy: 0.5375 - val_loss: 0.6854\nEpoch 25/30\n23/23 - 0s - 3ms/step - accuracy: 0.6313 - loss: 0.6122 - val_accuracy: 0.5250 - val_loss: 0.6871\nEpoch 26/30\n23/23 - 0s - 3ms/step - accuracy: 0.6285 - loss: 0.6113 - val_accuracy: 0.5375 - val_loss: 0.6822\nEpoch 27/30\n23/23 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6157 - val_accuracy: 0.5250 - val_loss: 0.6700\nEpoch 28/30\n23/23 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6102 - val_accuracy: 0.5375 - val_loss: 0.6811\nEpoch 29/30\n23/23 - 0s - 3ms/step - accuracy: 0.6299 - loss: 0.6109 - val_accuracy: 0.5375 - val_loss: 0.6891\nEpoch 30/30\n23/23 - 0s - 3ms/step - accuracy: 0.6341 - loss: 0.6100 - val_accuracy: 0.5375 - val_loss: 0.6857\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n              precision    recall  f1-score   support\n\n           0       0.61      0.25      0.36        91\n           1       0.58      0.86      0.69       109\n\n    accuracy                           0.58       200\n   macro avg       0.59      0.56      0.53       200\nweighted avg       0.59      0.58      0.54       200\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport yfinance as yf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load the scraped Reddit data\ndf = pd.read_csv('/kaggle/input/50kcomments/reddit_stock_data_20240921_225227.csv')\n\ndef preprocess_data(df):\n    # Convert dates to datetime using the correct format\n    df['created_utc'] = pd.to_datetime(df['created_utc'], format='%Y-%m-%d %H:%M:%S')\n    \n    # Combine title and body for sentiment analysis\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    \n    # Extract time-based features after converting datetime\n    df['day_of_week'] = df['created_utc'].dt.dayofweek  # 0=Monday, 6=Sunday\n    df['hour_of_day'] = df['created_utc'].dt.hour       # 0-23 hours\n    \n    return df\n\ndef analyze_sentiment(texts, batch_size=16):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=2,\n        torch_dtype=torch.float16\n    ).to(device)\n\n    dataset = TensorDataset(torch.arange(len(texts)))\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Analyzing sentiment\"):\n            batch_texts = [texts[i] for i in batch[0]]\n            inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            predictions.extend(preds.cpu().numpy().tolist())\n\n    return predictions\n\ndef get_stock_data(symbol, start_date, end_date):\n    stock_data = yf.download(symbol, start=start_date, end=end_date)\n    stock_data['returns'] = stock_data['Close'].pct_change()\n    return stock_data\n\ndef create_features(reddit_df, stock_df):\n    # Ensure 'day_of_week' and 'hour_of_day' are present in reddit_df after preprocessing\n    if 'day_of_week' not in reddit_df.columns or 'hour_of_day' not in reddit_df.columns:\n        raise KeyError(\"Columns ['day_of_week', 'hour_of_day'] are missing from reddit_df\")\n    \n    # Aggregate Reddit data by date and tickers\n    daily_reddit = reddit_df.groupby([reddit_df['created_utc'].dt.date, 'tickers']).agg({\n        'sentiment': 'mean',\n        'score': 'sum',\n        'num_comments': 'sum',\n        'day_of_week': 'first',   # Ensure the first day_of_week per group is taken\n        'hour_of_day': 'first'    # Ensure the first hour_of_day per group is taken\n    }).reset_index()\n    \n    stock_df['created_utc'] = stock_df.index.date\n    merged_df = pd.merge(daily_reddit, stock_df, left_on='created_utc', right_on='created_utc', how='inner')\n    \n    # Calculate moving averages\n    merged_df['MA_10'] = merged_df['Close'].rolling(window=10).mean()\n    merged_df['MA_50'] = merged_df['Close'].rolling(window=50).mean()\n    \n    # Create result column based on moving average crossover strategy\n    merged_df['result'] = (merged_df['MA_10'] > merged_df['MA_50']).astype(int)\n    \n    return merged_df.dropna()\n\ndef create_bert_model():\n    model = BertForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=1,  # Binary classification\n        output_attentions=False,\n        output_hidden_states=False,\n    )\n    return model\n\ndef train_model(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize BERT tokenizer\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n    \n    # Tokenize and encode sequences\n    def encode_sequences(texts):\n        return tokenizer.batch_encode_plus(\n            texts,\n            add_special_tokens=True,\n            max_length=128,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n    \n    # Encode training and testing data\n    train_encodings = encode_sequences(X_train.astype(str).apply(' '.join, axis=1))\n    test_encodings = encode_sequences(X_test.astype(str).apply(' '.join, axis=1))\n    \n    # Convert to PyTorch datasets\n    train_dataset = TensorDataset(\n        train_encodings['input_ids'],\n        train_encodings['attention_mask'],\n        torch.tensor(y_train.values, dtype=torch.float32)\n    )\n    test_dataset = TensorDataset(\n        test_encodings['input_ids'],\n        test_encodings['attention_mask'],\n        torch.tensor(y_test.values, dtype=torch.float32)\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n    \n    # Initialize model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = create_bert_model().to(device)\n    \n    # Set up optimizer\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    # Training loop\n    num_epochs = 3\n    for epoch in range(num_epochs):\n        model.train()\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluation\n    model.eval()\n    predictions = []\n    actual_labels = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            predictions.extend((logits.squeeze() > 0).cpu().numpy())\n            actual_labels.extend(labels.cpu().numpy())\n    \n    print(classification_report(actual_labels, predictions))\n    \n    return model\n\ndef main():\n    print(\"Preprocessing data...\")\n    reddit_df = preprocess_data(df)\n    \n    print(\"Analyzing sentiment...\")\n    reddit_df['sentiment'] = analyze_sentiment(reddit_df['full_text'].tolist())\n    \n    start_date = reddit_df['created_utc'].min().strftime('%Y-%m-%d')\n    end_date = reddit_df['created_utc'].max().strftime('%Y-%m-%d')\n    print(f\"Fetching stock data for AAPL from {start_date} to {end_date}...\")\n    stock_df = get_stock_data('AAPL', start_date, end_date)\n    \n    print(\"Creating features...\")\n    features_df = create_features(reddit_df, stock_df)\n    \n    X = features_df[['sentiment', 'score', 'num_comments', 'day_of_week', 'hour_of_day']]\n    y = features_df['result']\n    \n    print(\"Training model...\")\n    model = train_model(X, y)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:32:49.522755Z","iopub.execute_input":"2024-09-22T13:32:49.523128Z","iopub.status.idle":"2024-09-22T13:38:36.363756Z","shell.execute_reply.started":"2024-09-22T13:32:49.523091Z","shell.execute_reply":"2024-09-22T13:38:36.362800Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Preprocessing data...\nAnalyzing sentiment...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9329b2d0a3402388de381ad25d2dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c31c876ee749ababe8c02bacf65098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4492e944a241ff948e2facd5c6a031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac5a6c20fa742f797c02186a775763e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2397ca8150b84aaeb4c8602db0106604"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAnalyzing sentiment: 100%|██████████| 1454/1454 [04:42<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fetching stock data for AAPL from 2024-07-24 to 2024-09-21...\n","output_type":"stream"},{"name":"stderr","text":"[*********************100%***********************]  1 of 1 completed\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Creating features...\nTraining model...\n","output_type":"stream"},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1/3: 100%|██████████| 50/50 [00:15<00:00,  3.26it/s]\nEpoch 2/3: 100%|██████████| 50/50 [00:15<00:00,  3.27it/s]\nEpoch 3/3: 100%|██████████| 50/50 [00:15<00:00,  3.20it/s]\nEvaluating: 100%|██████████| 13/13 [00:01<00:00,  9.40it/s]","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00        91\n         1.0       0.55      1.00      0.71       109\n\n    accuracy                           0.55       200\n   macro avg       0.27      0.50      0.35       200\nweighted avg       0.30      0.55      0.38       200\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:41:53.558885Z","iopub.execute_input":"2024-09-22T13:41:53.559886Z","iopub.status.idle":"2024-09-22T13:41:53.584597Z","shell.execute_reply.started":"2024-09-22T13:41:53.559819Z","shell.execute_reply":"2024-09-22T13:41:53.583650Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  comment_comment_id                               comment_comment_body  \\\n0            lo9am0c  Totally depends on your financial situation, r...   \n1            lo981xc  If you’re nearing retirement, then it maybe be...   \n2            lo9brj6  I’m 63.  I know how to invest and trade. I buy...   \n3            lo9f92c                   Spy looks good, up 30% this year   \n4            lo9fi6l  I mean if you buy a SP500 ETF you automaticall...   \n\n   comment_comment_score comment_comment_author comment_comment_created_utc  \\\n0                      4             JuiceByYou         2024-09-21 19:57:15   \n1                      3         Alert_Cost_836         2024-09-21 19:42:33   \n2                      3           NativeDave63         2024-09-21 20:03:53   \n3                      1               doge_fps         2024-09-21 20:23:21   \n4                      1          Echo-Possible         2024-09-21 20:24:47   \n\n                             comment_tickers subreddit  post_id  \\\n0                                        NaN    stocks  1fmanog   \n1                                      NVDA,    stocks  1fmanog   \n2              I, I, ETFS, I, I, IRA, IRA, I    stocks  1fmanog   \n3                                        NaN    stocks  1fmanog   \n4  I, SP500, ETF, NVDA,, AAPL,, MSFT,, META.    stocks  1fmanog   \n\n                                               title  \\\n0  Do you sell stocks nearing retirement and move...   \n1  Do you sell stocks nearing retirement and move...   \n2  Do you sell stocks nearing retirement and move...   \n3  Do you sell stocks nearing retirement and move...   \n4  Do you sell stocks nearing retirement and move...   \n\n                                                body  score  num_comments  \\\n0  So... If you are nearing your retirement age,,...      0            27   \n1  So... If you are nearing your retirement age,,...      0            27   \n2  So... If you are nearing your retirement age,,...      0            27   \n3  So... If you are nearing your retirement age,,...      0            27   \n4  So... If you are nearing your retirement age,,...      0            27   \n\n          created_utc                author  \\\n0 2024-09-21 19:20:15  West-Bodybuilder-867   \n1 2024-09-21 19:20:15  West-Bodybuilder-867   \n2 2024-09-21 19:20:15  West-Bodybuilder-867   \n3 2024-09-21 19:20:15  West-Bodybuilder-867   \n4 2024-09-21 19:20:15  West-Bodybuilder-867   \n\n                                                 url                 tickers  \\\n0  https://www.reddit.com/r/stocks/comments/1fman...  NVDA, AAPL, META, MSFT   \n1  https://www.reddit.com/r/stocks/comments/1fman...  NVDA, AAPL, META, MSFT   \n2  https://www.reddit.com/r/stocks/comments/1fman...  NVDA, AAPL, META, MSFT   \n3  https://www.reddit.com/r/stocks/comments/1fman...  NVDA, AAPL, META, MSFT   \n4  https://www.reddit.com/r/stocks/comments/1fman...  NVDA, AAPL, META, MSFT   \n\n                                           full_text  day_of_week  \\\n0  Do you sell stocks nearing retirement and move...            5   \n1  Do you sell stocks nearing retirement and move...            5   \n2  Do you sell stocks nearing retirement and move...            5   \n3  Do you sell stocks nearing retirement and move...            5   \n4  Do you sell stocks nearing retirement and move...            5   \n\n   hour_of_day  sentiment  \n0           19          0  \n1           19          0  \n2           19          0  \n3           19          0  \n4           19          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_comment_id</th>\n      <th>comment_comment_body</th>\n      <th>comment_comment_score</th>\n      <th>comment_comment_author</th>\n      <th>comment_comment_created_utc</th>\n      <th>comment_tickers</th>\n      <th>subreddit</th>\n      <th>post_id</th>\n      <th>title</th>\n      <th>body</th>\n      <th>score</th>\n      <th>num_comments</th>\n      <th>created_utc</th>\n      <th>author</th>\n      <th>url</th>\n      <th>tickers</th>\n      <th>full_text</th>\n      <th>day_of_week</th>\n      <th>hour_of_day</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lo9am0c</td>\n      <td>Totally depends on your financial situation, r...</td>\n      <td>4</td>\n      <td>JuiceByYou</td>\n      <td>2024-09-21 19:57:15</td>\n      <td>NaN</td>\n      <td>stocks</td>\n      <td>1fmanog</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>So... If you are nearing your retirement age,,...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2024-09-21 19:20:15</td>\n      <td>West-Bodybuilder-867</td>\n      <td>https://www.reddit.com/r/stocks/comments/1fman...</td>\n      <td>NVDA, AAPL, META, MSFT</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>5</td>\n      <td>19</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>lo981xc</td>\n      <td>If you’re nearing retirement, then it maybe be...</td>\n      <td>3</td>\n      <td>Alert_Cost_836</td>\n      <td>2024-09-21 19:42:33</td>\n      <td>NVDA,</td>\n      <td>stocks</td>\n      <td>1fmanog</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>So... If you are nearing your retirement age,,...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2024-09-21 19:20:15</td>\n      <td>West-Bodybuilder-867</td>\n      <td>https://www.reddit.com/r/stocks/comments/1fman...</td>\n      <td>NVDA, AAPL, META, MSFT</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>5</td>\n      <td>19</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>lo9brj6</td>\n      <td>I’m 63.  I know how to invest and trade. I buy...</td>\n      <td>3</td>\n      <td>NativeDave63</td>\n      <td>2024-09-21 20:03:53</td>\n      <td>I, I, ETFS, I, I, IRA, IRA, I</td>\n      <td>stocks</td>\n      <td>1fmanog</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>So... If you are nearing your retirement age,,...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2024-09-21 19:20:15</td>\n      <td>West-Bodybuilder-867</td>\n      <td>https://www.reddit.com/r/stocks/comments/1fman...</td>\n      <td>NVDA, AAPL, META, MSFT</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>5</td>\n      <td>19</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lo9f92c</td>\n      <td>Spy looks good, up 30% this year</td>\n      <td>1</td>\n      <td>doge_fps</td>\n      <td>2024-09-21 20:23:21</td>\n      <td>NaN</td>\n      <td>stocks</td>\n      <td>1fmanog</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>So... If you are nearing your retirement age,,...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2024-09-21 19:20:15</td>\n      <td>West-Bodybuilder-867</td>\n      <td>https://www.reddit.com/r/stocks/comments/1fman...</td>\n      <td>NVDA, AAPL, META, MSFT</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>5</td>\n      <td>19</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lo9fi6l</td>\n      <td>I mean if you buy a SP500 ETF you automaticall...</td>\n      <td>1</td>\n      <td>Echo-Possible</td>\n      <td>2024-09-21 20:24:47</td>\n      <td>I, SP500, ETF, NVDA,, AAPL,, MSFT,, META.</td>\n      <td>stocks</td>\n      <td>1fmanog</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>So... If you are nearing your retirement age,,...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2024-09-21 19:20:15</td>\n      <td>West-Bodybuilder-867</td>\n      <td>https://www.reddit.com/r/stocks/comments/1fman...</td>\n      <td>NVDA, AAPL, META, MSFT</td>\n      <td>Do you sell stocks nearing retirement and move...</td>\n      <td>5</td>\n      <td>19</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}